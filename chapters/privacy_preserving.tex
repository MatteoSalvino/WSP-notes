\section{Privacy-preserving data mining}
An user makes query to a database. He is allowed to make statistical queries and is smart so that is able to infer information he should not know by repeatedly sending queries to the database. Our goal is to allow him to query the database but we want also to preserve the privacy of the database itself. To do that we will preprocess the database records using the \textbf{data sanitization} approach. The idea is to overwrite the information in the database with realist looking but false data, generated in a random way, of similar type. Some example of sanitization methods are to simply add random noise to the information retrieved from the database, reveal only summary statistics, output perturbation as in the previous case. We say that this process of sanitization is good if the information that we are asking are drawn by an unknown distribution, because in this way sanitization reveals only the distribution. Suppose that we use a sanitization process that adds a random noise to the data retrieved from the database. The problem is that an attacker might collect several responses asking always the same query to the database in order to leverage them to infer the real answer. So, it's not so easy to define the concept of privacy. Let's see some definitions :
\begin{itemize}
\item \textbf{Weak} : it means that no single database entry has been revealed.
\item \textbf{Stronger} : it means that no single piece of information is revealed.
\item \textbf{Strongest} : the adversary beliefs that the data have not changed.
\end{itemize}
For the latter case, there is a way to formalize the difference between two probability distributions which goes under the name of \textbf{Kullback-Leibler distance}. It's defined in the following way
\begin{center}
$D_{KL}(P \mid \mid Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)}$.
\end{center} 
If the distance is small then the two distributions are similar. It measures mutual information between the original and randomized databases. In particular, we will have a privacy breach if the database revealing some answer to a query significantly changes the attacker's probability that $Q(x_{i})$ is true, where $x_{i}$ denotes a record in the database and $Q(.)$ represent some property. In general, let $Q(x)$ be some property and $\rho_{1}, \rho_{2}$ be two probabilities such that the first is negligible and the second is non negligible, we can define two types of privacy breach :
\begin{itemize}
\item \textbf{Straight privacy breach} : $P(Q(x)) \leq \rho_{1}$, but $P(Q(x) \mid R(x) = y) \geq \rho_{2}$, i.e. $Q(x)$ is unlikely a priori, but likely after seeing the randomized value of $x$.
\item \textbf{Inverse privacy breach} : $P(Q(x)) \geq \rho_{2}$, but $P(Q(x) \mid R(x) = y) \leq \rho_{1}$, i.e. $Q(x)$ is likely a priori, but unlikely after seeing the randomized value of $x$.
\end{itemize}
We can formalize our problem by defining for each entry of the database a function such that each entry has an a priori belief and a posteriori belief after seeing a certain numbers of responses provided by the database. If the confidence on the responses doesn't changes at all, why do we make queries ? Instead if the confidence on the responses changes then the attacker is able to extract useful information from the database. In particular, there is an important theorem defined by Dinur and Nissim that says : let $n$ be the size of the database. If $O(n^{\frac{1}{2}})$ perturbation are applied, an adversary can extract the entire database after $poly(n)$ queries, but even with $O(n^{\frac{1}{2}} \log n)$ perturbation, it's unlikely that user can learn anything useful from the perturbated answer (too much noise). The problem is that to implement the strongest notion of privacy is too complex in practice. For this reason we need to work with a weaker privacy definition also called \textbf{differential privacy} that shows that it's possible to get positive results.

\subsection{Differential privacy}
Differential privacy states that whatever is learned about one respondent $A$ would be learned regardless of whether or not $A$ participates to the database. The idea is that the statistical outcome is indistinguishable regardless whether a particular user is included in the database. Formally, for every pair of databases $D_{1}$ and $D_{2}$ that differs in one row, for every output $O$, if an algorithm $A$ satisfies differential privacy then $\frac{Pr[A(D_{1}) = O]}{Pr[A(D_{2}) = O]} < \exp^{\epsilon}$, with $\epsilon > 0$. In other words, an adversary should not be able to use output $O$ to distinguish between any $D_{1}$ and $D_{2}$. Naturally we would like that such condition holds for all outputs, because we allow the user to make several queries to the database, and to be sure that he doesn't not learn anything. Suppose that our database has $n$ records. We can define $n + 1$ games where game $0$ represent the fact that the adversary interacts with the sanitized database when all the data are present, and game $i$ the fact that the adversary interacts with the sanitized database when all the data are present except for the record $i$. Next, the adversary makes a lot of queries and obtains some transcript $S$. At this point suppose that we modify the database changing just one record. The adversary will continue to send queries to this modified database and will get a transcript $S^{'}$. The two set of answers $S$ and $S^{'}$ doesn't allow the adversary to distinguish any significant information about the presence or absence of the record previously changed. In formal way, we will say that the sanitization process $San$ is $\epsilon$-indistinguishable if $\forall A, \forall DB, DB^{'}$ which differ in one row, $\forall S$ we have 
\begin{center}
$P(San(DB) \in S) \in (1 \pm \epsilon) P(San(DB^{'}) \in S)$.
\end{center}
This means that indistinguishability implies differential privacy. Indeed, we can say that $San$ is safe if for all prior distributions on $DB$ and for all transcripts $S$ the statistical distance between any pair of a posterior probability given $S$ is less or equal than $\epsilon$, i.e. their distributions are so close that we are not able to distinguish between the two. To implement this approach in practice we simply adds a noise to the sanitized answers such that each answer doesn't leak too much information about the database, but the noisy answers are close to the original answers so that they are useful to the user. Naturally, privacy depends on the noise that we decided to use. The larger the noise is the higher is the privacy and less useful an answer will be. This means that the noise value strictly depends on the records in the database. In particular, we will define the \textbf{global sensitivity} measure, which simply measure the differences in the data. There is a well known theorem claiming that if we adds to the real answer a noise generated from a Laplace distribution that is strictly depends from the global sensitivity measure and inversely proportional to $\epsilon$ that the resulting answer is $\epsilon$-indistinguishable. Now the question is how much noise do we need for privacy ? We can define it in formal way through the \textbf{sensitivity} measure. Consider a query $q : I \rightarrow R$ and be $S(q)$ the smallest number s. t. for any neighboring tables $D, D^{'}$ we have $\mid q(D) - q(D^{'}) \mid \leq S(q)$, If the sensitivity of the query is $S$, then the following guarantees $\epsilon$-differential privacy $\lambda = \frac{S}{\epsilon}$. The sensitivity metric measures the maximum deviation of a single database entry. However the problem is always the same, the higher the noise is the less interesting the answers will be (e.g. issues for count, sum, max and multiple queries). Another problem is that when we have non-numeric valued query, in which way should we answer ? The intuition is that we need to introduce some randomization in order to return only high quality elements, which are the ones close to the real answer. This approach guarantees privacy, but we know that error in the quality is given by the variance of the dataset. Instead in the Laplace mechanism the variance is much smaller since it doesn't depend on the size of the input. So, this approach may be good for count queries, but in general the Laplace mechanism has a lower variance and thus provides us better results. Now, lets consider a comparison between one-query against multiple-query. Let $\epsilon_{1} = 0.1$ be the privacy parameter. In the one-query scenario, the data analyst perform a single query. This means that the analyst would not be able to perform a second query over the data without risking a breach of the policy. In the multiple-query scenario, the data analyst performs a single query with $\epsilon_{1}$ privacy parameter. Then the analyst can also perform a second query say with $\epsilon_{2} = 0.2$. So, after the second query the overall privacy loss is $0.1 + 0.2 = 0.3$. The problem is that every time the analyst make a query he can get some information about the data, and making many queries he gets many information. So, this poses the fact that the number of queries that should be done must be bounded. Indeed a vast majority of records in a database of size $n$ can be reconstructed when $n \log n^{2}$ queries are answered by a statistical database, but even if each answer has been arbitrarily altered to have up to $O(\sqrt{n})$ error (Dinur Nissim result). In general, we can distinguish between two kinds of query composition :
\begin{itemize}
\item \textbf{Sequential composition} : if $M_{1}, ..., M_{k}$ are algorithms that access a private database $D$ such that each $M_{i}$ satisfies $\epsilon_{i}$-differential privacy, then running all $k$ algorithms sequentially satisfies $\epsilon$-differential privacy with $\epsilon = \epsilon_{1} + ... + \epsilon_{k}$.
\item \textbf{Parallel composition} : if $M_{1}, ..., M_{k}$ are algorithms that access a private database $D$ such that each $M_{i}$ satisfies $\epsilon_{i}$-differential privacy, then running all $k$ algorithms in parallel satisfies $\epsilon$-differential privacy with $\epsilon = \max \{\epsilon_{1} + ... + \epsilon_{k}\}$.
\end{itemize}
To solve the previous problem we can use the following approaches :
\begin{itemize}
\item \textbf{Direct} : the output of the algorithm is the list of query answers.
\item \textbf{Synthetic data} : the algorithm constructs a synthetic dataset $D^{'}$, which can be queries directly by the analyst, even though answers may not be accurate.
\end{itemize}

\subsubsection{Deploying differential privacy for 2020 Census}
The laws in the US requires that everything should be counted only once and in the right place. There are quite established rules about which data should be collected and made public (e.g. race/ethnicity, occupancy status and group quarters population). The data that should be collected must preserve individuals, neither the secretary nor any other officer may get information about such data. The data are collected only for statistical purposes, kept confidential and in any way they won't identifies individuals or establishments. Our goal is to avoid improper disclosure of the data. For instance the $2010$ Census collected data on $308$ million people, and the process for preserving privacy is the following : the raw data are collected from respondents, then the unduplicated data are selected and there is a process that implements some operations on the data in order to alter them and guarantee their confidentiality. Next the statistical data are computed and organized in tables. The problem is that publishing too many statistics result in the compromise of the entire database of confidential data, because we can setup a system of equations to reconstruct a significant portion (about $38 \%$) of the original data. However, this kind of attack is not perfect because the attacker would not know which re-identifications are correct. To solve this problem statistical agencies have just two choices : publish fewer statistics or publish statistics with less accuracy. The first option is not allowed because if we publish fewer statistics then we don't know when we have reached a threshold of having published too many statistics. This is because the information that is published can be combined with external information. We can just use the second option which leverages the differential privacy, to get a tradeoff between data accuracy and privacy loss. An important point is that if we go through the "noise barrier" several times for the same data, probably we will get different numbers. Another important aspect is that the noise has also an impact on the number of people that are on the statistics. For example, if there are more people then we can put a small error to avoid re-identification attacks. Whereas if we have just few people then the error should be high. The advantages of such approach are the following : the details can be explained to the public, we can tune the error parameters in order to reach different privacy levels, the privacy guarantees don't depends on external data, we are protected against re-identification attacks and protects every member of the population. However, we have also some challenges to face such as entire country must be processed at once for reaching the best accuracy, and every use of confidential data must be taken into account the privacy-loss budget. So we assume that every record in the population may be modified, but there is a global privacy budget that should be kept constant. This means that records in the tabulation data have no exact counterpart in the confidential data. The entire census is treated as a set of queries on histograms. For each geographic level (e.g state, country, etc.) confidential data is used to build the corresponding public histograms. Next, these histograms are post-processed to guarantee their consistency. The main issue in the US Census is the size of the blocks, because we know that for blocks with a small size we can build a system of equations to obtain information about people. A possible approach is to independently protect each block, with which the measure queries and the reconstruction is done on each block. The main advantages of this approach are the following : it simple and easy to parallelize, privacy cost doesn't depends on the number of blocks and releasing differential privacy for one block has the same cost as releasing it for all blocks. However, we will have significant error at higher level and the variance of each geographical unit is proportional to the number of blocks it contains. For this reason, another possibility is to use the top-down mechanism, which first generates national histogram without geographic identifiers and then allocates counts in histogram to each geography "top down". This approach is easy to parallelize, each geographical unit can have its own strategy selection, we have a parallel composition at each geo-level and we have also reduced the variance for many aggregate regions. To summarize what we have said so far we can say that :
\begin{itemize}
\item The fundamental law of information reconstruction tells use that if we publish too many statistics derived from a confidential data source with a high degree of accuracy, then after a finite number of queries we will completely expose the confidential data (Dinur and Nissim result). All the statistical disclosure limitation techniques tries to protect privacy by limiting the quantity of data released or by reducing the accuracy of the data itself.
\item When implementing differential privacy, the privacy-loss budget makes data accuracy and privacy competing uses of a finite resource (bits in the underlying data). It's impossible to protect privacy while also releasing highly accurate data to support every use case, and vice versa. The statistics for large populations can be protected with a negligible amount of noise, whereas to protect the statistics for small populations is required a significant amount of noise.
\item How we implement any disclosure avoidance strategy will impact the accuracy and usability of the resulting data. Indeed, the design of the system can often have a greater impact on the accuracy of the resulting data than the selection of the privacy-loss budget. With differential privacy, the amount of noise we must inject into the data strictly depend on the sensitivity of the calculation we are performing, because such sensitivity depends on the impact that the presence or absence of any individual could have on the resulting calculation. For this reason, some statistics typically require less noise than others.
\item We need to take into account that to guarantee differential privacy different tabulations of the same characteristic may not be internally consistent, because typically differential privacy inject noise from a symmetric distribution including fractional and negative values. However, converting such noisy values into non negative integers to guarantee consistency introduces more error into the data that is strictly necessary to protect privacy.
\end{itemize}

\subsection{Netflix prize dataset}
Netflix has started as a company that was online movie rental service. Indeed Netflix instead of renting movies start to deliver them on the Internet. In October $2006$, it released real movie ratings of $500k$ subscribers because they have a good recommendation system but they would like to improve it. So, they make a prize publishing data about $10 \%$ of all Netflix users, clearly removing names, perturbing rating information and taking the average user rated over $200$ movies. The goal was to predict how a user will rate a movie beating the Netflix's algorithm and gets a $1$ million prize. Analyzing the data, we see that the most popular movies were rated by almost half of the users, while the least popular just by few users. However, two researchers was able to deanonymize the released database. The idea is of fixing some target record $r$ in the original dataset, and the goal is to learn as much about $r$ as possible by collecting information from the released database. However, the problem is that the background knowledge (IMDB dataset) is nosy, only a small portions of the samples has been releases, there may be false matches and the records itself may be perturbed. Anyway, since ratings about least popular movies are very personalized (it's not so common that two users give the same rating for a not popular movie), the researchers found that, with this cross references on movie ratings and date of ratings, some users turned out to be members of both IMDB and Netflix and private information was obtained with a very low probability of error. Indeed, in average just $2$ ratings on not famous movies are sufficient to reduce the number of candidates to $8$, and only $4$ ratings are sufficient to uniquely identify the user in the database published by Netflix. The fundamental point is that we needs always to take into account the prior knowledge that an attacker may have to understand if our data really guarantees privacy or not.

\subsection{Social network deanonymization}
Given a network, we will say that such network is partially or totally deanonymized when the attacker, starting from a re-identification of a fraction of the nodes from an auxiliary network that partially overlaps with the original network, gains knowledge about the edges of the graph that represent the network (1-1 mapping between two graphs). For instance, we can consider the partial deanonymization of Twitter graph using the Flickr graph. In particular both companies shared mandatory username, optional name and location. Researchers were able to reach roughly $30.8 \%$ of the mappings correctly re-identified, $57 \%$ were not identified and $12.2 \%$ were identified incorrectly. Instead, if an active adversary is a member of a social network could deanonymize it in the following way : first of all he creates fake nodes if needed, and adds edges in the social network graph with features that easily help him to recognize himself in the anonymized version of the graph. Typically, the attacker have access to different network, whose membership partially overlaps the network targeted by the attacker. If it's not the case, the information that the attacker needs might be extracted from the original network with the automatic crawling approach using some malicious third-party application. In positive case, the deanonymization process is performed via a re-identification algorithm, which is constituted by two phases :
\begin{enumerate}
\item \textbf{Seed identification} : the algorithm identifies a small number of seed nodes in both target network and auxiliary network, mapping them with each other. In order to find this mapping, we assume a clique structure in the auxiliary network and search the same clique on the target network, using common neighbors and the nodes degree.
\item \textbf{Propagation} : in this phase the attacker builds a deterministic 1-1 mapping between the nodes of the target and auxiliary network. Each iteration starts with the accumulated mapping, and then an unmapped node $u$ in the target network is picked and a score for each unmapped node $v$ in the auxiliary network is computed. We want to underline that the algorithm finds new mappings using the topological structure of the network and the feedback from previous mappings.
\end{enumerate}

\subsection{Privacy concerns in social networks}
We know that social networks may appears useful because allows us to connect with our friends, post photos and videos, etc. However, people posting information by themselves on the social networks increase the probability to be subject to malwares, stalking, etc. leading to several privacy issues. All of this information about an user could be useful when combined with social engineering techniques to start an identity theft attack or to compromise sensitive information. Typically adversaries targets social network users to obtain information to be used in phishing and other types of attacks. Often the users are not aware of the amount of information that is shared in social networks. Another problem is that the facility with which a person can create a fake account on the social networks and to possibly lures other users in doing something wrong in order to compromise their privacy. For example Facebook in January $2012$ did an experiment over $600k$ users randomly selected, to determine the effect of the emotional alteration through the interactions of those users with Facebook itself. This is a clear example in which the information of each user to conduct the experiment is seen as a breach of privacy without the user disclosure. Another problem is that users accept people as "friends" that doesn't know at all just to enlarge the "friends group", but at the same time allows such people to access their personal information and pictures. We understand that the social network privacy doesn't exist within the realm of privacy settings, but the privacy control is within the hands of the users. Another interesting fact is that $69 \%$ of Facebook users believe that the information about other Facebook users reveal may create privacy risks for those users, but the majority of users think that the information about themselves don't represent a privacy risk at all.