\section{k-Anonymity and other cluster-based methods}
We know that a large amount of person-specific data has been collected in recent years from both governments and private entities. The information extracted from such data are used for analyze trends and patterns or to formulate public policies. There are laws and regulations that requires that some collected data must be made public (e.g Census data). The problem is that we want to publish these data because they might have an impact, for example Health-care datasets, but at the same time we want to preserve anonymity, i.e. such data must be used only for research purposes but they should not be used for other purposes. This aspect goes under the name of \textbf{inference control}. The idea is the following : we have individuals that submit data, these data are collected and we apply to them a masking process in order to remove the possibility of identify the people, and finally we publish the modified data so that researchers can use them. In particular in the masking process we have two conflicting goals : on one side we want to protect the confidentiality of individuals, on the other side we would like to preserve the utility of these data reducing as much as possible the information loss. First of all we would like to eliminate the person identifying information such as name, phone number, email, etc. However, this is not sufficient because we might release information using some additional information. The data we are talking about are also called \textbf{microdata}, and the resulting masked data are called \textbf{masked microdata}. Instead, all the computed data, for instance for statistics purposes, are called \textbf{macrodata}. The problem is that even if we do not publish the individual data, there may be some fields that may uniquely identify some individual, which an attacker can leverage them to identify to detect a specific person. The same reasoning is applied for macrodata. This process is also called \textbf{re-identification by linking}. For this reason, in order to protect macrodata, we might suppress some cells by removing their value. These are called \textbf{primary suppression}. However, this doesn't solve the problem because an attacker could guess with a certain probability the suppressed cell computing an interval that contains the target cell leveraging the marginal totals of the data. To block such inferences, additional cells may need to be suppressed (\textbf{secondary suppression}) to guarantee that the interval are sufficiently large, and thus to reduce the probability of the attacker to make the right guess. A \textbf{quasi-identifier} is an identifier that allows to uniquely identify a certain amount of the population. For example, the zip code, birth date and gender uniquely identify the $87 \%$ of the population in the U.S. To solve this problem we need to introduce the \textbf{K-Anonymity} concept. Its main idea is that when we publish some database, the information for each people contained in the released table cannot be distinguished from at least $k - 1$ individuals whose information also appears in the released table. In other words, any quasi-identifier present in the released table must appear in at least $k$ records. Naturally these $k$ record form an equivalence class. We can achieve $K$-Anonymity using two techniques : generalization in order to replace quasi-identifiers with less specific but still semantically consistent values, and suppression to not release a value at all. For example, to apply generalization we might suppress the last $2$ digits of the zip code, or the last digit of the age, etc. When we apply the generalization technique, we replace specific quasi-identifiers with less specific values until we get $k$ identical values. While we apply the suppression technique, when the generalization approach causes too much information loss. A possible problem is called \textbf{curse of dimensionality} in the sense that generalization that is used fundamentally relies on spatial locality, but real-world datasets has a huge number of attributes. The problem is that we are classified using too many dimensions that is almost impossible to escape out. However a problem that we could have is that some fields could allows the attacker to exclude a priori some of the k-anonymous records. Another problem arose when the record appear in the same order in the released table ans in the original table, allowing an attacker to perform an \textbf{unsorted matching attack}. A possible solution is simply to randomize the order before releasing. Another problem comes from the fact that different releases of the same private table can be linked together to compromise $K$-Anonymity allowing an attacker to perform a \textbf{complementary release attack}. An attacker could perform a \textbf{homogeneity attack} when a $3$-anonymous table contains a field that has the same value for some group of people. We understand that the simple definition of $K$-Anonymity is not sufficient. A possible solution is to consider the \textbf{l-diversity} approach, in which sensitive attributes must be "diverse" within each quasi-identifier equivalence class. However this might not be sufficient, because it doesn't avoid an attacker to perform a probabilistic inference attack. Suppose that only $1 \%$ are affected by a specific disease and we have $10000$ records. In this case there can be at most $100$ people that are affected by such disease. Now, if we are able to make random queries to the database, we might expect that only $1 \%$ of the people in the query response will be affected by such disease. Clearly, we have other parameters that should count, and so we might found among those parameters some queries that might allows us to detect a specific person. This kind of attack goes under the name of \textbf{skewness attack}. This means that distinct $l$-diversity might be unachievable in practice, because making more queries there would be always some category in which there could only few people. We might generalize so much to eliminate all the useful information, but we want to publish useful information that increase our knowledge. The concept of \textbf{T-closeness} measure the semantic distance between concepts, requiring the generalization of sensitive information. However this doesn't prevent an attacker that knows some additional information that is not quasi-identifier to identify. $K$-Anonymity and the other approaches we discussed before are not useful at all, because they focuses on data transformation, but not on what can be learned from the anonymized dataset. Furthermore, it's subject to the curse of dimensionality problem and several attacks discussed before. The $K$-Anonymity good points are that it's very simple to implement and there are a wide variety of algorithms for implementing it. The major advantages of $l$-diversity are that it provides a greater distribution of sensitive attributes with the group increasing the data protection, and provides protection against attribute disclosure (it's an enhancement of $K$-Anonymity technique). However, we already said that it can be unachievable in practice and it's subject to several attacks such as swekness and similarity attacks. Finally, the $T$-closeness major benefits are that it interrupts attribute disclosure to protect data privacy, protects against homogeneity and background knowledge attacks and identifies the semantic closeness of attributes. However, it also has some disadvantages such as necessitates that sensitive attributes are spread in the equivalence class to be close to that in the overall table and using EMD measure it's very hard to identify the closeness between $t$-value and the knowledge gained.

\subsection{HIPAA privacy rules}
These rules have been proposed in the US. The idea is that to publish statistical health data only for legal and economic issues. For example smoking and probability of cancer, etc. In particular, these rules says that the covered entities must remove all of a list of $18$ enumerated identifiers and have no actual knowledge that the information remaining could be used, alone or in combination, to identify a subject of the information. The identifier that must be removed include direct identifiers such as name, street address, SSN, birth date, zip code, etc. They allows the initial three digits of a zip code if the geographic unit formed by combining all zip codes with the same initial three digits contains more than $20000$ people. In addition, age (if less than $90$), gender, ethnicity and other demographic information may remain in the information. They are intended to provide to covered entities with a simple, definitive method that doesn't require much judgment by the covered entity to determine if the information are correctly de-identified. However, their main disadvantage is that do not guarantee us to reach $K$-anonymity.

\subsection{UK confidentiality guidance}
UK in order to ensure the privacy of sensitive data started to use a different approach wrt to HIPAA privacy rules. First of all we try to understand the users requirements for the publication of data. Then we try to understand the key characteristic of these data and if there would be the possibility of disclosure of such data. We need to take into account which are the dangers that can be done by attacker leveraging the disclosure of such data. If required, we select an appropriate disclosure control methods to manages this risk. The idea is that this process is not purely syntactic because we need to do some reasoning on the data. For instance, suppose there is an attacker with a special interest in statistics discover from a table that only a small number of very young women live in a particular area. The small number in the cell doesn't tell the intruder who the women are, but it may leverages other sources of information to locate the individuals and disclose more details. This situation may occur when small values are reported for particular cells. In a large population the effort and expertise required in order to discover more details about the statistical unit may be very high. Naturally, as the population moves to smaller geographical area it becomes easier to find units and discover information. In particular, the risk categories are defined in terms of the likelihood of an attempt to identify individuals, and the impact of any identification. We can distinguish the following categories :
\begin{itemize}
\item \textbf{Medium risk} : it will be sufficient to consider all cells of size $1$ or $2$ unsafe. Care should be also taken where a row or column is dominated by zeros.
\item \textbf{High risk} : the likelihood of an identification attempt will be higher and the impact of any successful identification would be great. All the cells of size $1$ to $4$ are considered unsafe and care should be taken where a row or column is dominated by zeros. Higher levels of protection may be required for small geographical levels or for particular variables with an extremely high level of interest and impact.
\end{itemize}
A possible approach to minimize the number of unsafe cells and preserve the original counts is called \textbf{table redesign}. Its main idea is to group the categories within a table aggregating to a higher geographical level or for a larger population subgroup and to aggregates tables across higher geographic level (e.g. zip code) or a number of years/months (e.g. age). Another approach is the \textbf{cell values modification} schema, which provides the following features :
\begin{itemize}
\item \textbf{Cell suppression} : in this way unsafe cells are not published.
\item \textbf{Rounding} : it adjust the values in all cells in a table to a specified base. This creates uncertainty about the real value for any cell.
\item \textbf{Barnardisation} : it adjust the cells of every table by $+1, 0, -1$ according to the probabilities.
\end{itemize}
Another approach is to adjust the data by swapping pairs of record within a micro-dataset that are partially matched to alter the geographical locations attached to the record, but at the same time leave all the other aspects unchanged.